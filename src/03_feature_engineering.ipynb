{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9fa2a8b-bf46-41eb-8624-ce8238edfef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 03 - feature engineering\n",
    "\n",
    "Time for feature engineering. Because we're going to look at scaling inputs as part of the modelling process, we don't cover that here.\n",
    "\n",
    "We're looking at the following flow:\n",
    "\n",
    "<img src=\"../docs/imgs/energy-sa-feature-engineering.png \" width=\"300\">\n",
    "\n",
    "So we're performing the following steps:\n",
    "- Obtaining and processing the UK bank holidays data from the gov site, you'll learn to work with json content.\n",
    "- Identifying from our data which region of the UK each LV feeder belongs to. We're going to obtain some geographical data that includes the relevant administrative boundaries for the UK, then join it based on if an LV feeder is within that region using DBSQLs spatial features.\n",
    "- We introduce and join our weather data based on both time and nearest point to get weather features included.\n",
    "- We look at normalizing our dependant variable based on the number of properties served by each LV feeder, then examine the impact of outliers and scale.\n",
    "- We calculate some lagged values using window functions to use as regressors in our model.\n",
    "- We introduce some basic frequency domain features by calculating the cyclical positions of different time periods. For example a weekly cycle can be thought of as 7 distinct steps, which can be mapped onto a unit circle with a `sin()` and `cos()` function to give the -1 to 1 scale feature for position-in-cycle.\n",
    "\n",
    "Finally, we split our output into test, train and holdout. We holdout a small sample of LV feeders entirely for blind testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f48acae0-774f-4f78-a9a6-4016977c5556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./includes/common_functions_and_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17d7b1aa-606f-468a-8bd6-51d91e8b9fc0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load data and configs"
    }
   },
   "outputs": [],
   "source": [
    "source_table_name_energy = (\n",
    "    f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.smart_meter_data_clean\"\n",
    ")\n",
    "source_table_name_weather = (\n",
    "  f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.weather_data_clean\"\n",
    ")\n",
    "\n",
    "if not spark.catalog.tableExists(source_table_name_energy) or not spark.catalog.tableExists(source_table_name_weather):\n",
    "  dbutils.notebook.exit('One of our sources does not exist')\n",
    "\n",
    "clean_meter_df = spark.table(source_table_name_energy)\n",
    "weather_pivoted_df = spark.table(source_table_name_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9778020b-308f-4854-b8b0-cb3e4083aefd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "target_table_name_administrative_boundaries_ref = f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.gb_administrative_boundaries\"\n",
    "target_table_name_uk_bank_holidays = f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.uk_bank_holidays\"\n",
    "target_table_name_unique_locations_with_admin_boundaries = f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.energy_locations_with_admin_boundaries\"\n",
    "\n",
    "target_table_name_half_hourly_statistics = f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.half_hourly_statistics_and_bounds\"\n",
    "target_table_name_ref_unique_feeders_with_holdout = f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.ref_unique_feeders_with_holdout\"\n",
    "\n",
    "target_table_name_holdout_features = f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.unscaled_holdout_features\"\n",
    "target_table_name_train = f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.unscaled_train_features\"\n",
    "target_table_name_test = f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.unscaled_test_features\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0bcfd3d4-3293-4bec-94c5-e250c800a261",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Public holidays and geographical processing\n",
    "\n",
    "In order to make better forecasts, we can derive some very simple features to include in our model.\n",
    "\n",
    "Public holidays would be a great example of one such feature, but the UK is, of course, a union of four different countries and therefore holidays differ by region ðŸ˜¬\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7a793cb-e850-434f-8faf-58db6ee24a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Part 1: The UK is multiple countries\n",
    "\n",
    "Luckily there is a well-maintained public dataset of administrative boundaries hosted as part of the [Overture Maps](https://overturemaps.org/) project and we can use these to allocate each of our feeder locations to a particular country.\n",
    "\n",
    "We can go ahead and read the 'division areas' data from Overture, directly from their S3 bucket. We'll use the new Databricks Spatial SQL functions to simplify these geometries a little and further speed up our imminent join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8891eb52-3617-483f-9970-120d06f3a5fb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get the Overture reference for administrative boundaries"
    }
   },
   "outputs": [],
   "source": [
    "overture_release = \"2025-03-19.0\"\n",
    "divisions_theme = \"divisions\"\n",
    "divisions_type = \"division_area\"\n",
    "\n",
    "overture_divisions_uri = f\"s3a://overturemaps-us-west-2/release/{overture_release}/theme={divisions_theme}/type={divisions_type}/\"\n",
    "\n",
    "#Only re-read if we're set to overwrite or the table doesn't exist.\n",
    "if not spark.catalog.tableExists(target_table_name_administrative_boundaries_ref) or CONFIG.overwrite_data:\n",
    "  (\n",
    "      spark.read.parquet(overture_divisions_uri)\n",
    "      .where(F.col(\"subtype\") == \"region\")\n",
    "      .where(\"country = 'GB'\")\n",
    "      .selectExpr(\n",
    "          \"st_aswkb(st_simplify(st_geomfromwkb(geometry), 0.001)) as region_geom\",\n",
    "          \"region as region_name\",\n",
    "      )\n",
    "      .write.saveAsTable(\n",
    "          target_table_name_administrative_boundaries_ref, mode=\"overwrite\"\n",
    "      )\n",
    "  )\n",
    "\n",
    "divisions_df = spark.table(target_table_name_administrative_boundaries_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5061fce-19b2-4ff0-b67b-0de7efe2a0d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For our next trick, we're going to save an intermediate result, which is the unique meter locations with the administrative boundaries.\n",
    "\n",
    "We do this because the geometry join (st_contains) is not currently supported by photon, and later on we're going to leverage photon for our time series preparation.\n",
    "\n",
    "The result is approximately 50K unique locations (x, y coordinates) and their respective admin boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08627d5b-2302-4354-9f67-c8dc621637fd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set up the join to augment our meters data"
    }
   },
   "outputs": [],
   "source": [
    "join_expression_sql = F.expr(\n",
    "    \"\"\"\n",
    "st_contains(\n",
    "  st_geomfromwkb(region_geom), \n",
    "  st_point(locations.geometry.x, locations.geometry.y)\n",
    ")\n",
    "\"\"\"\n",
    ")\n",
    "if (not spark.catalog.tableExists(target_table_name_unique_locations_with_admin_boundaries)) or CONFIG.overwrite_data:\n",
    "  (\n",
    "      clean_meter_df.select('geometry').distinct().alias(\"locations\")\n",
    "      .join(divisions_df, on=join_expression_sql, how=\"inner\")\n",
    "      .drop(\"region_geom\")\n",
    "      .write.saveAsTable(\n",
    "          target_table_name_unique_locations_with_admin_boundaries, mode=\"overwrite\")\n",
    "  )\n",
    "\n",
    "energy_locations_with_admin_boundaries = spark.table(target_table_name_unique_locations_with_admin_boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b3318e7-8ae5-4b42-8642-3846f68eac53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Part 2: Add in the holidays and weekends\n",
    "\n",
    "The UK website conveniently provides all the bank holidays in json format, here: [https://www.gov.uk/bank-holidays.json](https://www.gov.uk/bank-holidays.json)\n",
    "\n",
    "We're going to do the following:\n",
    "- use pandas `read_json` to grab this straight from the web.\n",
    "- This produces a dataframe with columns for each region and 2 rows:\n",
    "  - row 1 is a duplicated region name (called 'division'), which we wil drop.\n",
    "  - row 2 which contains the json content of bank holidays for that region.\n",
    "- We melt this frame to pivot it into two columns:\n",
    "  - region\n",
    "  - Array of structs (our json content)\n",
    "- Now we have 2 columns x 3 rows for each region in the UK.\n",
    "- We turn this into a spark frame, then use the magical [inline](https://docs.databricks.com/aws/en/sql/language-manual/functions/inline) command to automatically explode and expand our json content out into 1 row per holiday x N columns of struct data.\n",
    "\n",
    "**The result is:**\n",
    "7 columns by 252 rows of holidays.\n",
    "\n",
    "Finally just for good measure, we'll add a column indicating if this is a weekend as we'd expect a deviation in domestic consumption patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2773bf7-ac83-4222-ac37-1678cf168a0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if not spark.catalog.tableExists(target_table_name_uk_bank_holidays) or CONFIG.overwrite_data:\n",
    "\n",
    "  pdf = pd.read_json('https://www.gov.uk/bank-holidays.json')\n",
    "  bank_holidays_df = spark.createDataFrame(pdf.drop('division').melt()).withColumnsRenamed({'variable': 'division', 'value':'holidays_array'})\n",
    "\n",
    "  lookup_location_df = spark.createDataFrame(\n",
    "    [\n",
    "      ('GB-ENG', 'england-and-wales'),\n",
    "      ('GB-NIR', 'northern-ireland'),\n",
    "      ('GB-SCT', 'scotland'),\n",
    "      ('GB-WLS', 'wales')\n",
    "      ], 'region_name string, division string'\n",
    "    )\n",
    "\n",
    "  bank_holidays_df = bank_holidays_df.join(lookup_location_df, on='division', how='left')\n",
    "  # We're going to use inline here to seamlessly process this json from many-rows-and-columns in a single cell per row into a normal table.\n",
    "  bank_holidays_df.selectExpr('division','region_name','inline(holidays_array)', 'True as is_bank_holiday').write.saveAsTable(target_table_name_uk_bank_holidays, mode='overwrite')\n",
    "\n",
    "bank_holidays_df = spark.table(target_table_name_uk_bank_holidays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab14e0bc-0d0f-4a5a-b5d9-e8b2dc28faeb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Add in bank holidays and weekends"
    }
   },
   "outputs": [],
   "source": [
    "energy_data_with_features = (\n",
    "    clean_meter_df.withColumn(\n",
    "        \"date\", F.to_date(\"data_collection_log_timestamp\")\n",
    "    )\n",
    "    .alias(\"meter\")\n",
    "    .join(energy_locations_with_admin_boundaries, on=\"geometry\", how='inner')\n",
    "    .alias('meter_with_admin_boundaries')\n",
    "    .join(\n",
    "        bank_holidays_df.select(\n",
    "            \"date\", \"region_name\", \"is_bank_holiday\", \"title\"\n",
    "        ).alias(\"hols\"),\n",
    "        on=[\"date\", \"region_name\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"half_hour\",\n",
    "        F.hour(\"data_collection_log_timestamp\") * 60\n",
    "        + F.minute(\"data_collection_log_timestamp\"),\n",
    "    ).withColumn('hour', F.hour(\"data_collection_log_timestamp\"))\n",
    "    .withColumn(\"is_bank_holiday\", F.col(\"is_bank_holiday\").isNotNull())\n",
    "    .withColumn(\n",
    "        \"is_weekend\", F.weekday(\"data_collection_log_timestamp\") >= 5\n",
    "    )  # 0 indexed on monday, so 5 = saturday\n",
    "    .withColumn('weekday_monday_eq_0', F.weekday(\"data_collection_log_timestamp\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0ee1349-622d-4194-a3d5-dcc19eb7cc69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Part 3: add in weather data based on geography and nearest time interval\n",
    "\n",
    "So we have some energy meter data and some weather historicals. Now we need to join on two quite complex things: Geography (nearest weather measurement point to LV feeder), and time period (half hourly meter meets hourly weather).\n",
    "\n",
    "Luckily, we made a simplification earlier! We collected our weather historicals at 0.25 deg intervals. We can use this to do a lazy-neighbour approach for our LV feeders to massively simplify our geo-problem, so we'll round our LV feeder locations to the nearest 0.25 degrees.\n",
    "\n",
    "For our time alignment we can use the same approach and round to the nearest hour. We could also use a window function to interpolate the weather values to half hourly periods, but this would be another chunk of processing and would double the size of our weather dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe50e9e-567d-4e86-8c02-b479a8af5719",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Join energy with weather based on time and geo-location"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheSgKICAgIGVuZXJneV9kYXRhX3dpdGhfZmVhdHVyZXMud2l0aENvbHVtbigKICAgICAgICAicm91bmRlZF94IiwgRi5yb3VuZChGLmNvbCgiZ2VvbWV0cnkueCIpIC8gMC4yNSkgKiAwLjI1CiAgICApCiAgICAud2l0aENvbHVtbigicm91bmRlZF95IiwgRi5yb3VuZChGLmNvbCgiZ2VvbWV0cnkueSIpIC8gMC4yNSkgKiAwLjI1KQogICAgLndpdGhDb2x1bW4oCiAgICAgICAgInJvdW5kZWRfdGltZXN0YW1wIiwKICAgICAgICBGLmRhdGVfdHJ1bmMoImhvdXIiLCBGLmNvbCgiZGF0YV9jb2xsZWN0aW9uX2xvZ190aW1lc3RhbXAiKSksCiAgICApCiAgICAuYWxpYXMoImVuZXJneSIpCiAgICAuam9pbigKICAgICAgICB3ZWF0aGVyX3Bpdm90ZWRfZGYuYWxpYXMoImxvY2F0aW9ucyIpLAogICAgICAgIG9uPVtGLmNvbCgnZW5lcmd5LnJvdW5kZWRfeCcpID09IEYuY29sKCdsb2NhdGlvbnMueCcpLCBGLmNvbCgnZW5lcmd5LnJvdW5kZWRfeScpID09IEYuY29sKCdsb2NhdGlvbnMueScpLCBGLmNvbCgnZW5lcmd5LnJvdW5kZWRfdGltZXN0YW1wJykgPT0gRi5jb2woJ2xvY2F0aW9ucy50aW1lJyldLAogICAgICAgIGhvdz0ibGVmdCIsCiAgICApLndoZXJlKCdsdl9mZWVkZXJfdW5pcXVlX2lkID09ICJTU0VOLTQ5MDYwMDYyMzAwMSInKQop\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView12d94bb\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView12d94bb\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView12d94bb\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView12d94bb) SELECT `data_collection_log_timestamp`,SUM(`total_consumption_active_import`) `column_7b5c753866`,SUM(`t2m`) `column_7b5c753885` FROM q GROUP BY `data_collection_log_timestamp`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView12d94bb\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "data_collection_log_timestamp",
             "id": "column_7b5c753863"
            },
            "y": [
             {
              "column": "total_consumption_active_import",
              "id": "column_7b5c753866",
              "transform": "SUM"
             },
             {
              "column": "t2m",
              "id": "column_7b5c753885",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_7b5c753866": {
             "type": "line",
             "yAxis": 0
            },
            "column_7b5c753869": {
             "type": "line",
             "yAxis": 1
            },
            "column_7b5c753875": {
             "type": "line",
             "yAxis": 1
            },
            "column_7b5c753885": {
             "type": "line",
             "yAxis": 1
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "logarithmic"
            },
            {
             "opposite": true,
             "type": "linear"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "8dc088af-32c3-4519-b49e-cb0a178119a4",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 5.1015625,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "data_collection_log_timestamp",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "data_collection_log_timestamp",
           "type": "column"
          },
          {
           "alias": "column_7b5c753866",
           "args": [
            {
             "column": "total_consumption_active_import",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "alias": "column_7b5c753885",
           "args": [
            {
             "column": "t2m",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "energy_data_with_weather_join = (\n",
    "    energy_data_with_features\n",
    "      .withColumn(\"rounded_x\", F.round(F.col(\"geometry.x\") / 0.25) * 0.25)\n",
    "      .withColumn(\"rounded_y\", F.round(F.col(\"geometry.y\") / 0.25) * 0.25)\n",
    "      .withColumn(\"rounded_timestamp\",\n",
    "                  F.date_trunc(\"hour\", F.col(\"data_collection_log_timestamp\"))\n",
    "                 )\n",
    "    .alias(\"energy\")\n",
    "    .join(\n",
    "      weather_pivoted_df.alias(\"locations\"),\n",
    "      on=[\n",
    "          F.col(\"energy.rounded_x\") == F.col(\"locations.x\"),\n",
    "          F.col(\"energy.rounded_y\") == F.col(\"locations.y\"),\n",
    "          F.col(\"energy.rounded_timestamp\") == F.col(\"locations.time\"),\n",
    "      ],\n",
    "      how=\"left\",\n",
    "    )\n",
    "    # Calculate the euclidean distance between coordinates, may be useful to compare against prediction errors and weather.\n",
    "    .withColumn('approx_distance_to_weather_station', F.sqrt((F.col('x')-F.col('geometry.x'))**2 + (F.col('y')-F.col('geometry.y'))**2))\n",
    "    # Tidy up column order and names : keys, dependant variable, features...\n",
    "    .select(\n",
    "        \"lv_feeder_unique_id\",\n",
    "        \"secondary_substation_unique_id\",\n",
    "        \"dno_alias\",\n",
    "        \"data_collection_log_timestamp\",\n",
    "        \"hour\",\n",
    "        \"half_hour\",\n",
    "        F.col(\"time\").alias(\"weather_forecast_time\"),\n",
    "        F.col(\"geometry\").alias(\"lv_feeder_geometry\"),\n",
    "        F.struct(\"x\", \"y\").alias(\"weather_forecast_geometry\"),\n",
    "        \"approx_distance_to_weather_station\",\n",
    "        \"aggregated_device_count_active\",\n",
    "        \"total_consumption_active_import\",\n",
    "        \"is_weekend\",\n",
    "        \"weekday_monday_eq_0\",\n",
    "        \"is_bank_holiday\",\n",
    "        F.col(\"title\").alias(\"bank_holiday_name\"),\n",
    "        \"t2m\",\n",
    "        \"u10\",\n",
    "        \"v10\",\n",
    "        \"ssrd\",\n",
    "        \"strd\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6d85350-dbc6-4588-b9cd-7479fdcba60e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Watch out for: rows with no available weather data. It's possible depending on your dataset that you would end up with a bunch of a rows that you have no weather data to join to, which you would need to decide how to handle. A cheap and lazy method would be to inner join the two and cut off any results lacking data, but you may miss a section of your time data you're interested in (maybe you need to use forecast weather for the latest smart meter readings?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95ee3300-f990-4107-89f0-6d1742327851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preparing our time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8216998f-2d99-4e2d-8640-e0c361504c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Basic dependant variable processing\n",
    "\n",
    "The very first thing we need to do wih our consumption figures is normalise them against the number of active devices for each low voltage feeder. Currently they're all on different scales depending how many devices are served.\n",
    "\n",
    "From our data exploration we know the minimum value of `aggregated_device_count_active` is 5, so we don't anticipate any infinite responses.\n",
    "\n",
    "There is two plots below showing the difference pre- and post- normalization. It should be pretty apparent that pre-normalization would lead you to believe that every LV_feeder has a very different profile.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3940c65-a4ab-4728-8ec7-22e1fc82382a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query to visualise the difference with normalization"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShzcGFyay5zcWwoCiAgZiIiIgp3aXRoIGZpcnN0X25fZmVlZGVycyBhcyAoCiAgc2VsZWN0IGRpc3RpbmN0CiAgICBsdl9mZWVkZXJfdW5pcXVlX2lkCiAgZnJvbQogICAge3NvdXJjZV90YWJsZV9uYW1lX2VuZXJneX0KICBsaW1pdCAxMAopCnNlbGVjdAogIG1kLmx2X2ZlZWRlcl91bmlxdWVfaWQsCiAgZGF0ZV90cnVuYygnREFZJywgZGF0YV9jb2xsZWN0aW9uX2xvZ190aW1lc3RhbXApIGFzIGRhdGUsCiAgc3VtKHRvdGFsX2NvbnN1bXB0aW9uX2FjdGl2ZV9pbXBvcnQpIGFzIGRhaWx5X2NvbnN1bXB0aW9uLAogIHN1bSh0b3RhbF9jb25zdW1wdGlvbl9hY3RpdmVfaW1wb3J0KQogIC8gZmlyc3QoYWdncmVnYXRlZF9kZXZpY2VfY291bnRfYWN0aXZlKSBhcyBkYWlseV9jb25zdW1wdGlvbl9ub3JtYWxpemVkCmZyb20KICB7c291cmNlX3RhYmxlX25hbWVfZW5lcmd5fSBtZAogICAgaW5uZXIgam9pbiBmaXJzdF9uX2ZlZWRlcnMKICAgICAgb24gbWQubHZfZmVlZGVyX3VuaXF1ZV9pZCA9IGZpcnN0X25fZmVlZGVycy5sdl9mZWVkZXJfdW5pcXVlX2lkCmdyb3VwIGJ5CiAgbWQubHZfZmVlZGVyX3VuaXF1ZV9pZCwKICBkYXRlCiIiIgopKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView83bfd5f\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView83bfd5f\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView83bfd5f\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView83bfd5f) SELECT `date`,SUM(`daily_consumption`) `column_60e9e63837`,`lv_feeder_unique_id` FROM q GROUP BY `date`,`lv_feeder_unique_id`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView83bfd5f\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "non-normalized",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "lv_feeder_unique_id",
             "id": "column_60e9e63839"
            },
            "x": {
             "column": "date",
             "id": "column_60e9e63834"
            },
            "y": [
             {
              "column": "daily_consumption",
              "id": "column_60e9e63837",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": false,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_60e9e63837": {
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "6ae1c156-bb7d-4d7d-afc2-914f496a19fd",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 1.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "date",
           "type": "column"
          },
          {
           "column": "lv_feeder_unique_id",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "date",
           "type": "column"
          },
          {
           "alias": "column_60e9e63837",
           "args": [
            {
             "column": "daily_consumption",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "lv_feeder_unique_id",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShzcGFyay5zcWwoCiAgZiIiIgp3aXRoIGZpcnN0X25fZmVlZGVycyBhcyAoCiAgc2VsZWN0IGRpc3RpbmN0CiAgICBsdl9mZWVkZXJfdW5pcXVlX2lkCiAgZnJvbQogICAge3NvdXJjZV90YWJsZV9uYW1lX2VuZXJneX0KICBsaW1pdCAxMAopCnNlbGVjdAogIG1kLmx2X2ZlZWRlcl91bmlxdWVfaWQsCiAgZGF0ZV90cnVuYygnREFZJywgZGF0YV9jb2xsZWN0aW9uX2xvZ190aW1lc3RhbXApIGFzIGRhdGUsCiAgc3VtKHRvdGFsX2NvbnN1bXB0aW9uX2FjdGl2ZV9pbXBvcnQpIGFzIGRhaWx5X2NvbnN1bXB0aW9uLAogIHN1bSh0b3RhbF9jb25zdW1wdGlvbl9hY3RpdmVfaW1wb3J0KQogIC8gZmlyc3QoYWdncmVnYXRlZF9kZXZpY2VfY291bnRfYWN0aXZlKSBhcyBkYWlseV9jb25zdW1wdGlvbl9ub3JtYWxpemVkCmZyb20KICB7c291cmNlX3RhYmxlX25hbWVfZW5lcmd5fSBtZAogICAgaW5uZXIgam9pbiBmaXJzdF9uX2ZlZWRlcnMKICAgICAgb24gbWQubHZfZmVlZGVyX3VuaXF1ZV9pZCA9IGZpcnN0X25fZmVlZGVycy5sdl9mZWVkZXJfdW5pcXVlX2lkCmdyb3VwIGJ5CiAgbWQubHZfZmVlZGVyX3VuaXF1ZV9pZCwKICBkYXRlCiIiIgopKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView87d7c15\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView87d7c15\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView87d7c15\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView87d7c15) SELECT `date`,SUM(`daily_consumption_normalized`) `column_60e9e63843`,`lv_feeder_unique_id` FROM q GROUP BY `date`,`lv_feeder_unique_id`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView87d7c15\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "normalized",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "lv_feeder_unique_id",
             "id": "column_60e9e63845"
            },
            "x": {
             "column": "date",
             "id": "column_60e9e63840"
            },
            "y": [
             {
              "column": "daily_consumption_normalized",
              "id": "column_60e9e63843",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": false,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_60e9e63843": {
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "46e0737f-5c5f-421e-acfc-134b45addaa6",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 1.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "date",
           "type": "column"
          },
          {
           "column": "lv_feeder_unique_id",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "date",
           "type": "column"
          },
          {
           "alias": "column_60e9e63843",
           "args": [
            {
             "column": "daily_consumption_normalized",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "lv_feeder_unique_id",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.sql(\n",
    "  f\"\"\"\n",
    "with first_n_feeders as (\n",
    "  select distinct\n",
    "    lv_feeder_unique_id\n",
    "  from\n",
    "    {source_table_name_energy}\n",
    "  limit 10\n",
    ")\n",
    "select\n",
    "  md.lv_feeder_unique_id,\n",
    "  date_trunc('DAY', data_collection_log_timestamp) as date,\n",
    "  sum(total_consumption_active_import/1000) as daily_consumption,\n",
    "  sum(total_consumption_active_import/1000)\n",
    "  / first(aggregated_device_count_active) as daily_consumption_normalized\n",
    "from\n",
    "  {source_table_name_energy} md\n",
    "    inner join first_n_feeders\n",
    "      on md.lv_feeder_unique_id = first_n_feeders.lv_feeder_unique_id\n",
    "group by\n",
    "  md.lv_feeder_unique_id,\n",
    "  date\n",
    "\"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fcd0ae7-7f2f-4eb9-b60b-703c3c296ed4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Normalize our data"
    }
   },
   "outputs": [],
   "source": [
    "normalized_df = energy_data_with_weather_join.withColumn(\n",
    "    \"normalized_consumption_kwh\",\n",
    "    F.try_divide(\n",
    "        F.col(\"total_consumption_active_import\"),\n",
    "        F.col(\"aggregated_device_count_active\"),\n",
    "    )/1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca4062c9-8118-4de9-aa0c-1c310fb6984c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If we had access to more information about LV feeders (such as model, installed capacity, max properties served, etc), we could look at performing some more advanced analytics. An LV feeder is a good independant variable - The details would be readily accessible to a supplier, so we will always have access to it. If it turns out that different classes of feeder have different characteristics (for example, we cannot have EV charger consumption on model xyz), then this becomes a meaningful predictor for consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2770a358-359c-4a59-b431-220d907ef038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adding in lagged values\n",
    "\n",
    "Lagged values are useful for regression models, and since we're dealing with half hourly periods may act as useful features for identifying patterns ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d840ec6a-f0e6-4e7a-80d3-4d7bca4202a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"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\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView145e3ca\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView145e3ca\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView145e3ca\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView145e3ca) SELECT `data_collection_log_timestamp`,`diffed` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView145e3ca\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "data_collection_log_timestamp",
             "id": "column_82ccc419168"
            },
            "y": [
             {
              "column": "diffed",
              "id": "column_82ccc419170"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "scatter",
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "diffed": {
             "type": "scatter",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "9d536c98-ff64-4a64-b52e-16c6450863b5",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 5.5054408482142865,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "selects": [
          {
           "column": "data_collection_log_timestamp",
           "type": "column"
          },
          {
           "column": "diffed",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_window = Window.partitionBy(\"lv_feeder_unique_id\").orderBy(\n",
    "    \"data_collection_log_timestamp\"\n",
    ")\n",
    "consumption_column = \"normalized_consumption_kwh\"\n",
    "lagged_df = normalized_df.withColumn(\n",
    "    \"lagged_values\",\n",
    "    F.struct(\n",
    "        (\n",
    "            F.lag(\n",
    "                consumption_column, 1, default=F.col(consumption_column)\n",
    "            ).over(my_window)\n",
    "        ).alias(\"lag_1\"),\n",
    "        (\n",
    "            F.lag(\n",
    "                consumption_column, 2, default=F.col(consumption_column)\n",
    "            ).over(my_window)\n",
    "        ).alias(\"lag_2\"),\n",
    "        (\n",
    "            F.lag(\n",
    "                consumption_column, 3, default=F.col(consumption_column)\n",
    "            ).over(my_window)\n",
    "        ).alias(\"lag_3\"),\n",
    "    ),\n",
    ").withColumn(\n",
    "    \"diffed_timestamp\",\n",
    "        (\n",
    "            F.col(\"data_collection_log_timestamp\")\n",
    "            - F.lag(\n",
    "                \"data_collection_log_timestamp\",\n",
    "                1,\n",
    "                default=F.col(\"data_collection_log_timestamp\"),\n",
    "            ).over(my_window)\n",
    "        ).cast('int')\n",
    "        \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb4b3a15-7238-4a81-a0ce-2751f00a0ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adding some cyclical indicators\n",
    "\n",
    "| cyclical component | periods |\n",
    "|-|-|\n",
    "|half-hour | 48 |\n",
    "|hour| 24|\n",
    "|day|7|\n",
    "|week|52|\n",
    "|month|12|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a89a91aa-271f-4a79-9fd1-d2081d3ea26e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_cyclical_features(df, col_name, period):\n",
    "    angle = F.col(col_name) * (2 * F.pi() / period)\n",
    "    return df.withColumn(f\"{col_name}_sin\", F.sin(angle)).withColumn(\n",
    "        f\"{col_name}_cos\", F.cos(angle)\n",
    "    )\n",
    "\n",
    "# Standardise all column names for cyclicals, and set them all to start at zero then run to their respective periods:\n",
    "\n",
    "lagged_df_with_cyclicals = (\n",
    "    lagged_df.withColumn(\"cyc_halfhour\", F.col(\"half_hour\") / 30)\n",
    "    .withColumnRenamed(\"hour\", \"cyc_hour\")\n",
    "    .withColumnRenamed(\"weekday_monday_eq_0\", \"cyc_day\")\n",
    "    .withColumn(\"cyc_week\", F.weekofyear(\"data_collection_log_timestamp\") - 1)\n",
    "    .withColumn(\"cyc_month\", F.month(\"data_collection_log_timestamp\") - 1)\n",
    ")\n",
    "cyclical_features = {\"cyc_halfhour\":48, \"cyc_hour\":24, \"cyc_day\":7, \"cyc_week\":52, \"cyc_month\":12}\n",
    "\n",
    "for col_name, period in cyclical_features.items():\n",
    "  lagged_df_with_cyclicals = add_cyclical_features(lagged_df_with_cyclicals, col_name, period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "764c691f-e508-4384-8a71-a2f06de775cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShsYWdnZWRfZGZfd2l0aF9jeWNsaWNhbHMp\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewaa6702a\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewaa6702a\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewaa6702a\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewaa6702a) SELECT DATE_TRUNC('MINUTE',`data_collection_log_timestamp`) `column_a3b7f857422`,SUM(`cyc_halfhour_sin`) `column_a3b7f857409`,SUM(`normalized_consumption_kwh`) `column_a3b7f857415`,SUM(`cyc_month_sin`) `column_a3b7f857418` FROM q GROUP BY `column_a3b7f857422`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewaa6702a\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "data_collection_log_timestamp",
             "id": "column_a3b7f857422",
             "transform": "MINUTE_LEVEL"
            },
            "y": [
             {
              "column": "cyc_halfhour_sin",
              "id": "column_a3b7f857409",
              "transform": "SUM"
             },
             {
              "column": "normalized_consumption_kwh",
              "id": "column_a3b7f857415",
              "transform": "SUM"
             },
             {
              "column": "cyc_month_sin",
              "id": "column_a3b7f857418",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_a3b7f857409": {
             "type": "line",
             "yAxis": 0
            },
            "column_a3b7f857412": {
             "type": "line",
             "yAxis": 0
            },
            "column_a3b7f857415": {
             "type": "line",
             "yAxis": 0
            },
            "column_a3b7f857418": {
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 1744664097681,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": [
        [
         "table",
         10000
        ]
       ],
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "c746bfe4-7b71-43c5-b536-095be549bff9",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 22.875,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 1744663953593,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "column_a3b7f857422",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "column_a3b7f857422",
           "args": [
            {
             "column": "data_collection_log_timestamp",
             "type": "column"
            },
            {
             "string": "MINUTE",
             "type": "string"
            }
           ],
           "function": "DATE_TRUNC",
           "type": "function"
          },
          {
           "alias": "column_a3b7f857409",
           "args": [
            {
             "column": "cyc_halfhour_sin",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "alias": "column_a3b7f857415",
           "args": [
            {
             "column": "normalized_consumption_kwh",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "alias": "column_a3b7f857418",
           "args": [
            {
             "column": "cyc_month_sin",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 1744663953527,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(lagged_df_with_cyclicals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d326a458-9ea8-400b-80b4-4df10fba15b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Thinking about outliers (again)\n",
    "\n",
    "we've previously removed outliers very loosely with an aim at keeping statistically significant data in.\n",
    "\n",
    "We've picked up on a few more - Notably:\n",
    "- 3 LV feeders with values surpassing 35kWh in a HH period after being normalized for device counts.\n",
    "- periods with unexplainable anomalous behaviour - For example filtering the normalized usage to >10K will show you LV feeders on a scatter plot which appear to just linearly increase continually for a number of weeks on end. We've included one example below.\n",
    "\n",
    "While we suspect these may be erroneous readings, we have no way of easily isolating them, or investigating further at a device level.\n",
    "\n",
    "Fun activity: Zoom into the trending increases, and you get what appear to be valid consumption patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97aafe49-6ba5-47b1-a78b-80ea294fcb70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShub3JtYWxpemVkX2RmLmZpbHRlcigibHZfZmVlZGVyX3VuaXF1ZV9pZCA9ICdTU0VOLTM3MjEwMDE1MDAwMSciKSk=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewe77f017\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewe77f017\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewe77f017\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewe77f017) SELECT DATE_TRUNC('MINUTE',`data_collection_log_timestamp`) `column_82ccc419136`,AVG(`normalized_active_import`) `column_82ccc419133` FROM q GROUP BY `column_82ccc419136`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewe77f017\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "data_collection_log_timestamp",
             "id": "column_a3b7f857391",
             "transform": "MINUTE_LEVEL"
            },
            "y": [
             {
              "column": "normalized_consumption_kwh",
              "id": "column_82ccc419133",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": false,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "percentValues": false,
            "stacking": null
           },
           "seriesOptions": {
            "column_82ccc419133": {
             "type": "line",
             "yAxis": 0
            },
            "column_82ccc419138": {
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "fce87c1c-3adf-473b-bc84-87532338fa95",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 5.65289306640625,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "column_a3b7f857391",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "column_a3b7f857391",
           "args": [
            {
             "column": "data_collection_log_timestamp",
             "type": "column"
            },
            {
             "string": "MINUTE",
             "type": "string"
            }
           ],
           "function": "DATE_TRUNC",
           "type": "function"
          },
          {
           "alias": "column_82ccc419133",
           "args": [
            {
             "column": "normalized_consumption_kwh",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(normalized_df.filter(\"lv_feeder_unique_id = 'SSEN-372100150001'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ae98f1f-911c-42d7-abe8-7d8531e4f551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We also have a situation where we know removal methods like the zscore would be unfairly influenced by outliers, and we can't assume that the IQR would work.\n",
    "\n",
    " If we plotted a histogram of the log-scale consumption (below) we'd see it's comically left skewed towards the 'hundreds' order of magnitude (2) - This makes sense, we're dealing with electrical consumption, and most of the day people arn't running multi-kW draw devices.\n",
    "\n",
    " We can see however, if we look at the change in log scale over time represented as a percentage, how usage shifts and peaks around 6-6:30pm, and the fraction of responses at magnitude 'thousands' (or kW draw) is much higher.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "037145ec-0573-4ad3-bb39-d684668bc4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheSgKICAgIG5vcm1hbGl6ZWRfZGYud2l0aENvbHVtbigKICAgICAgICAiaGgiLAogICAgICAgIEYuaG91cigiZGF0YV9jb2xsZWN0aW9uX2xvZ190aW1lc3RhbXAiKSAqIDYwCiAgICAgICAgKyBGLm1pbnV0ZSgiZGF0YV9jb2xsZWN0aW9uX2xvZ190aW1lc3RhbXAiKSwKICAgICkKICAgIC53aXRoQ29sdW1uKCJMb2dfc2NhbGVfaW1wb3J0IiwgRi5yb3VuZChGLmxvZzEwKCJub3JtYWxpemVkX2FjdGl2ZV9pbXBvcnQiKSkpCiAgICAuZ3JvdXBCeSgnaGgnLCdMb2dfc2NhbGVfaW1wb3J0JykuYWdnKEYuY291bnQoJyonKS5hbGlhcygnY291bnQnKSkKKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView7f2921c\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView7f2921c\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView7f2921c\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView7f2921c) SELECT `hh`,`Log_scale_import`,`count` FROM q\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView7f2921c\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "histogram",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "colorScheme": "YlOrRd",
           "columnConfigurationMap": {
            "x": {
             "column": "Log_scale_import",
             "id": "column_82ccc419185"
            },
            "y": [
             {
              "column": "count",
              "id": "column_82ccc419187",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 10,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "Log_scale_import": {
             "type": "column",
             "yAxis": 0
            },
            "column_82ccc419142": {
             "type": "column",
             "yAxis": 0
            },
            "column_82ccc419187": {
             "name": "Count of records",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "b370d84a-6192-45aa-b18d-7b3315d724c9",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 5.6534423828125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "Log_scale_import",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "Log_scale_import",
           "type": "column"
          },
          {
           "alias": "column_82ccc419187",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheSgKICAgIG5vcm1hbGl6ZWRfZGYud2l0aENvbHVtbigKICAgICAgICAiaGgiLAogICAgICAgIEYuaG91cigiZGF0YV9jb2xsZWN0aW9uX2xvZ190aW1lc3RhbXAiKSAqIDYwCiAgICAgICAgKyBGLm1pbnV0ZSgiZGF0YV9jb2xsZWN0aW9uX2xvZ190aW1lc3RhbXAiKSwKICAgICkKICAgIC53aXRoQ29sdW1uKCJMb2dfc2NhbGVfaW1wb3J0IiwgRi5yb3VuZChGLmxvZzEwKCJub3JtYWxpemVkX2FjdGl2ZV9pbXBvcnQiKSkpCiAgICAuZ3JvdXBCeSgnaGgnLCdMb2dfc2NhbGVfaW1wb3J0JykuYWdnKEYuY291bnQoJyonKS5hbGlhcygnY291bnQnKSkKKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView9de066a\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView9de066a\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView9de066a\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView9de066a) SELECT `hh`,SUM(`count`) `column_82ccc419155`,`Log_scale_import` FROM q GROUP BY `Log_scale_import`,`hh`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView9de066a\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Change in log scale response over time",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "Log_scale_import",
             "id": "column_82ccc419157"
            },
            "x": {
             "column": "half_hour",
             "id": "column_82ccc419199"
            },
            "y": [
             {
              "column": "count",
              "id": "column_82ccc419155",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": false,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "percentValues": true,
            "stacking": null
           },
           "seriesOptions": {
            "column_82ccc419150": {
             "type": "column",
             "yAxis": 0
            },
            "column_82ccc419155": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "title": {
             "text": "Hour"
            },
            "type": "-"
           },
           "yAxis": [
            {
             "title": {
              "text": "Fraction at log scale"
             },
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "dc825c0f-6965-4713-b7f7-843568283671",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 6.6534423828125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "half_hour",
           "type": "column"
          },
          {
           "column": "Log_scale_import",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "half_hour",
           "type": "column"
          },
          {
           "alias": "column_82ccc419155",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "Log_scale_import",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    normalized_df.dropna(how=\"any\", subset=[consumption_column])\n",
    "    .withColumn(\"Log_scale_import\", F.round(F.log10(consumption_column)))\n",
    "    .groupBy((F.col('half_hour')/60).alias('half_hour'),'Log_scale_import').agg(F.count('*').alias('count'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8174d64-3fca-449d-b266-d95808a1fb6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "So we're going to take a slightly different approach.\n",
    "\n",
    "We're going to create a reference table with the IQR limits calculated for **each half hour**, based on both:\n",
    "- normalized consumption value, and,\n",
    "- log10 normalized consumption value\n",
    "\n",
    "This will give us bounds we can join into our datasets and use if we like. We also get some useful statistics like 'how many data points do we have for each half-an-hour'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdca76b8-2ef8-47b8-b8b7-d31f1c961f8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheSgKICAgIG5vcm1hbGl6ZWRfZGYud2l0aENvbHVtbigid2VlayIsIEYud2Vla29meWVhcigiZGF0YV9jb2xsZWN0aW9uX2xvZ190aW1lc3RhbXAiKSkKICAgIC5ncm91cEJ5KCJ3ZWVrIikKICAgIC5hZ2coCiAgICAgICAgRi5jb3VudCgibm9ybWFsaXplZF9hY3RpdmVfaW1wb3J0IikuYWxpYXMoIm5zYW1wbGVzIiksCiAgICAgICAgRi5wZXJjZW50aWxlKCJub3JtYWxpemVkX2FjdGl2ZV9pbXBvcnQiLCAwLjI1KS5hbGlhcygicDI1IiksCiAgICAgICAgRi5wZXJjZW50aWxlKCJub3JtYWxpemVkX2FjdGl2ZV9pbXBvcnQiLCAwLjc1KS5hbGlhcygicDc1IiksCiAgICAgICAgKEYucGVyY2VudGlsZSgibm9ybWFsaXplZF9hY3RpdmVfaW1wb3J0IiwgMC43NSkgLSBGLnBlcmNlbnRpbGUoIm5vcm1hbGl6ZWRfYWN0aXZlX2ltcG9ydCIsIDAuMjUpKS5hbGlhcygiaXFyIiksCiAgICAgICAgRi5tZWFuKCJub3JtYWxpemVkX2FjdGl2ZV9pbXBvcnQiKS5hbGlhcygibWVhbiIpLAogICAgICAgIEYuc3RkZGV2KCJub3JtYWxpemVkX2FjdGl2ZV9pbXBvcnQiKS5hbGlhcygic3RkZGV2IiksCiAgICAgICAgRi5taW4oIm5vcm1hbGl6ZWRfYWN0aXZlX2ltcG9ydCIpLmFsaWFzKCJtaW4iKSwKICAgICAgICBGLm1heCgibm9ybWFsaXplZF9hY3RpdmVfaW1wb3J0IikuYWxpYXMoIm1heCIpLAogICAgICAgIEYubWVkaWFuKCJub3JtYWxpemVkX2FjdGl2ZV9pbXBvcnQiKS5hbGlhcygibWVkaWFuIiksCiAgICAgICAgRi5jb2woJ3AyNScpIC0gMS41KkYuY29sKCdpcXInKS5hbGlhcygnbG93ZXJfYm91bmQnKSwKICAgICAgICBGLmNvbCgncDc1JykgKyAxLjUqRi5jb2woJ2lxcicpLmFsaWFzKCd1cHBlcl9ib3VuZCcpCiAgICApCiAgICAub3JkZXJCeSgid2VlayIpCik=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView1a8365b\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView1a8365b\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView1a8365b\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView1a8365b) SELECT `week`,SUM(`mean`) `column_b1bbed03118`,SUM(`median`) `column_b1bbed03121`,SUM(`(lateralAliasReference(p25) - (1.5 * lateralAliasReference(iqr) AS lower_bound))`) `column_b1bbed03124`,SUM(`(lateralAliasReference(p75) + (1.5 * lateralAliasReference(iqr) AS upper_bound))`) `column_b1bbed03127` FROM q GROUP BY `week`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView1a8365b\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "week",
             "id": "column_b1bbed03116"
            },
            "y": [
             {
              "column": "mean",
              "id": "column_b1bbed03118",
              "transform": "SUM"
             },
             {
              "column": "median",
              "id": "column_b1bbed03121",
              "transform": "SUM"
             },
             {
              "column": "(lateralAliasReference(p25) - (1.5 * lateralAliasReference(iqr) AS lower_bound))",
              "id": "column_b1bbed03124",
              "transform": "SUM"
             },
             {
              "column": "(lateralAliasReference(p75) + (1.5 * lateralAliasReference(iqr) AS upper_bound))",
              "id": "column_b1bbed03127",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_b1bbed03118": {
             "name": "Mean",
             "type": "line",
             "yAxis": 0
            },
            "column_b1bbed03121": {
             "name": "Median",
             "type": "line",
             "yAxis": 0
            },
            "column_b1bbed03124": {
             "name": "Lower bound",
             "type": "line",
             "yAxis": 0
            },
            "column_b1bbed03127": {
             "name": "Upper bound",
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "581ecd64-92c8-4754-9726-21eb8a14046f",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 5.654541015625,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "week",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "week",
           "type": "column"
          },
          {
           "alias": "column_b1bbed03118",
           "args": [
            {
             "column": "mean",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "alias": "column_b1bbed03121",
           "args": [
            {
             "column": "median",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "alias": "column_b1bbed03124",
           "args": [
            {
             "column": "(lateralAliasReference(p25) - (1.5 * lateralAliasReference(iqr) AS lower_bound))",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "alias": "column_b1bbed03127",
           "args": [
            {
             "column": "(lateralAliasReference(p75) + (1.5 * lateralAliasReference(iqr) AS upper_bound))",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hh_statistics = (\n",
    "    lagged_df_with_cyclicals.groupBy('cyc_halfhour')\n",
    "    .agg(\n",
    "        # bounds\n",
    "        F.percentile(consumption_column, 0.25).alias(\"p25\"),\n",
    "        F.percentile(consumption_column, 0.75).alias(\"p75\"),\n",
    "        (\n",
    "            F.percentile(consumption_column, 0.75)\n",
    "            - F.percentile(consumption_column, 0.25)\n",
    "        ).alias(\"iqr\"),\n",
    "        (F.col(\"p25\") - 1.5 * F.col(\"iqr\")).alias(\"lower_bound\"),\n",
    "        (F.col(\"p75\") + 1.5 * F.col(\"iqr\")).alias(\"upper_bound\"),\n",
    "        # stats\n",
    "        F.mean(consumption_column).alias(\"mean\"),\n",
    "        F.stddev(consumption_column).alias(\"stddev\"),\n",
    "        F.min(consumption_column).alias(\"min\"),\n",
    "        F.max(consumption_column).alias(\"max\"),\n",
    "        F.median(consumption_column).alias(\"median\"),\n",
    "        F.count(consumption_column).alias(\"nsamples\"),\n",
    "        # logscale outliers\n",
    "        F.percentile(F.log10(consumption_column), 0.25).alias(\"log10_p25\"),\n",
    "        F.percentile(F.log10(consumption_column), 0.75).alias(\"log10_p75\"),\n",
    "        (\n",
    "            F.percentile(F.log10(consumption_column), 0.75)\n",
    "            - F.percentile(F.log10(consumption_column), 0.25)\n",
    "        ).alias(\"log10_iqr\"),\n",
    "        (F.col(\"log10_p25\") - 1.5 * F.col(\"log10_iqr\")).alias(\"log10_lower_bound\"),\n",
    "        (F.col(\"log10_p75\") + 1.5 * F.col(\"log10_iqr\")).alias(\"log10_upper_bound\"),\n",
    "    )\n",
    "    .orderBy(\"cyc_halfhour\")\n",
    "    .select(\n",
    "        \"cyc_halfhour\",\n",
    "        F.struct(\"p25\", \"p75\", \"iqr\", \"lower_bound\", \"upper_bound\").alias(\n",
    "            \"hh_bounds\"\n",
    "        ),\n",
    "        F.struct(\"mean\", \"stddev\", \"min\", \"max\", \"median\", \"nsamples\").alias(\n",
    "            \"hh_stats\"\n",
    "        ),\n",
    "        F.struct(\"log10_p25\", \"log10_p75\", \"log10_iqr\", \"log10_lower_bound\", \"log10_upper_bound\").alias(\n",
    "            'log10_bounds'\n",
    "    )\n",
    "))\n",
    "if not spark.catalog.tableExists(target_table_name_half_hourly_statistics) or CONFIG.overwrite_data:\n",
    "    (hh_statistics.write.option('overwriteSchema',True).saveAsTable(target_table_name_half_hourly_statistics, mode='overwrite'))\n",
    "\n",
    "hh_statistics = spark.table(target_table_name_half_hourly_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6c2d0dd-23ff-4243-a803-62868d6afe6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The final feature we're adding then is an indicator of if this is a suspected outlier. We'll keep outliers in the saved data and leave it to user discretion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bc7487c-714c-47dc-b80d-c246fedf6790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "featured_engineered_all_data = lagged_df_with_cyclicals.join(\n",
    "    hh_statistics.select(\"cyc_halfhour\", \"log10_bounds\"), on=\"cyc_halfhour\", how=\"inner\"\n",
    ").withColumn(\n",
    "    \"in_log10_iqr_limit\",\n",
    "    (F.col(\"normalized_consumption_kwh\") >= 10 ** F.col(\"log10_bounds.log10_lower_bound\"))\n",
    "    & (\n",
    "        F.col(\"normalized_consumption_kwh\")\n",
    "        <= 10 ** F.col(\"log10_bounds.log10_upper_bound\")\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffb02606-a37e-44d1-b946-fbedc2958e19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Splitting our data and saving the results\n",
    "\n",
    "We're going to do a couple things here:\n",
    "1. Reserve a fraction of LV feeder data traces entirely as a hold out validation.\n",
    "2. Split the remaining data into a standard split based on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b807e12-f013-449f-a157-61e458e76613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"dW5pcXVlX2x2X2ZlZWRlcnMgPSBmZWF0dXJlZF9lbmdpbmVlcmVkX2FsbF9kYXRhLnNlbGVjdCgnbHZfZmVlZGVyX3VuaXF1ZV9pZCcpLmRpc3RpbmN0KCkKaG9sZG91dF9sdl9mZWVkZXJzLCByZW1haW5pbmdfZGF0YSA9IHVuaXF1ZV9sdl9mZWVkZXJzLnJhbmRvbVNwbGl0KFswLjA1LCAwLjk1XSwgc2VlZD00MikKCmhvbGRvdXRfbHZfZmVlZGVycyA9IGhvbGRvdXRfbHZfZmVlZGVycy53aXRoQ29sdW1uKCdob2xkb3V0X2ZlZWRlcicsIEYubGl0KFRydWUpKQpyZW1haW5pbmdfZGF0YSA9IHJlbWFpbmluZ19kYXRhLndpdGhDb2x1bW4oJ2hvbGRvdXRfZmVlZGVyJywgRi5saXQoRmFsc2UpKQpob2xkb3V0X2ZlZWRlcl9yZWZlcmVuY2UgPSByZW1haW5pbmdfZGF0YS51bmlvbkJ5TmFtZShob2xkb3V0X2x2X2ZlZWRlcnMpCmRpc3BsYXkoaG9sZG91dF9mZWVkZXJfcmVmZXJlbmNlKQoKIyBob2xkb3V0X2RhdGEgPSBmZWF0dXJlZF9lbmdpbmVlcmVkX2FsbF9kYXRhLmpvaW4=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView2e6da1e\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView2e6da1e\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView2e6da1e\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView2e6da1e) SELECT `holdout_feeder`,COUNT(DISTINCT `lv_feeder_unique_id`) `column_a3b7f857121` FROM q GROUP BY `holdout_feeder`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView2e6da1e\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "holdout_feeder",
             "id": "column_a3b7f857120"
            },
            "y": [
             {
              "column": "lv_feeder_unique_id",
              "id": "column_a3b7f857121",
              "transform": "COUNT_DISTINCT"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_a3b7f857121": {
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "429136a6-5d64-4f6e-aafd-8ef89824425f",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 7.0030517578125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "holdout_feeder",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "holdout_feeder",
           "type": "column"
          },
          {
           "alias": "column_a3b7f857121",
           "args": [
            {
             "column": "lv_feeder_unique_id",
             "type": "column"
            }
           ],
           "function": "COUNT_DISTINCT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_lv_feeders = featured_engineered_all_data.select('lv_feeder_unique_id').distinct()\n",
    "holdout_lv_feeders, remaining_data = unique_lv_feeders.randomSplit([0.05, 0.95], seed=42)\n",
    "\n",
    "holdout_lv_feeders = holdout_lv_feeders.withColumn('holdout_feeder', F.lit(True))\n",
    "remaining_data = remaining_data.withColumn('holdout_feeder', F.lit(False))\n",
    "\n",
    "# save our table down for faster processing and use later.\n",
    "if not spark.catalog.tableExists(target_table_name_ref_unique_feeders_with_holdout) or CONFIG.overwrite_data:\n",
    "  remaining_data.unionByName(holdout_lv_feeders).write.saveAsTable(target_table_name_ref_unique_feeders_with_holdout, mode='overwrite')\n",
    "  \n",
    "holdout_feeder_reference = spark.table(target_table_name_ref_unique_feeders_with_holdout)\n",
    "\n",
    "# This gives us around 6k holdout feeders and 116k included feeders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9574687d-d7e4-4b60-8eb0-95e3c58e9b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "holdout_feature_data = featured_engineered_all_data.join(\n",
    "    holdout_feeder_reference.filter(\"holdout_feeder\"),\n",
    "    on=\"lv_feeder_unique_id\", how='inner')\n",
    "\n",
    "remaining_feature_data = featured_engineered_all_data.join(\n",
    "    holdout_feeder_reference.filter(\"not holdout_feeder\"),\n",
    "    on=\"lv_feeder_unique_id\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "# Training data: from 2024-01-01 to 2024-12-31\n",
    "train_data = remaining_feature_data.orderBy(\"data_collection_log_timestamp\").filter(\n",
    "    \"data_collection_log_timestamp between '2024-01-01' and '2024-12-31'\"\n",
    ")\n",
    "# Testing data: from 2025-01-01 to 2025-03-29\n",
    "test_data = remaining_feature_data.orderBy(\"data_collection_log_timestamp\").filter(\n",
    "    \"data_collection_log_timestamp between '2025-01-01' and '2025-03-29'\"\n",
    ")\n",
    "\n",
    "datasets_to_write = {\n",
    "  target_table_name_holdout_features: holdout_feature_data,\n",
    "  target_table_name_train: train_data,\n",
    "  target_table_name_test: test_data\n",
    "}\n",
    "\n",
    "for dataset_name, dataset in datasets_to_write.items():\n",
    "  if not spark.catalog.tableExists(dataset_name) or CONFIG.overwrite_data:\n",
    "    print(f\"writing: {dataset_name}\")\n",
    "    dataset.write.option('overwriteSchema', True).saveAsTable(dataset_name, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c23b705b-c23d-473a-a751-9a18f9e0f0e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1436459421571210,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
