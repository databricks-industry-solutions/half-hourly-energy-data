{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7f56b1a-6bf6-4dcf-ba2b-e1ea8ea94f17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Data preparation\n",
    "\n",
    "This notebook does some basic pre-processing to get data ready for feature engineering.\n",
    "\n",
    "We're looking at the central part of this diagram:\n",
    "\n",
    "<img src=\"../docs/imgs/energy-sa-clean-up-flow.png \" width=\"300\">\n",
    "\n",
    "## Clean up for energy data\n",
    "\n",
    "From our 02_interactive_exploration notebook, and the guidance from Weave, we have the following situations:\n",
    "\n",
    "| Problem | Applied solution |\n",
    "| - | - |\n",
    "|Extreme outliers beyond a value of 200,000|Remove data points, no imputing |\n",
    "| Timestamps not aligned to 30 minute increments | Round to nearest 30 minutes|\n",
    "| Missing data | ignore, handle in modelling prep |\n",
    "| duplicate timestamps | Take average value to deduplicate |\n",
    "\n",
    "## Data augmentation\n",
    "- Since our energy data is already in the relevant format, all we're doing is cleaning it up a little bit\n",
    "- For weather data, we need to pivot the dataset to get one row per location + timestamp combination with all variable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9426afa4-50b0-4d01-b59a-fd8d57cad58e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./includes/common_functions_and_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39b79d03-9c18-449b-bc8a-2cccd82116ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_table_name_energy = (\n",
    "    f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.smart_meter_data_raw\"\n",
    ")\n",
    "source_table_name_weather_history = (\n",
    "  f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.weather_data_raw\"\n",
    ")\n",
    "source_table_name_weather_forecast = (\n",
    "  f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.weather_forecast_raw\"\n",
    ")\n",
    "\n",
    "if not all(spark.catalog.tableExists(t) for t in [\n",
    "  source_table_name_energy,\n",
    "  source_table_name_weather_history,\n",
    "  source_table_name_weather_forecast\n",
    "  ]):\n",
    "  dbutils.notebook.exit('One of our sources does not exist')\n",
    "\n",
    "target_table_name_energy_data = f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.smart_meter_data_clean\"\n",
    "target_table_name_weather_history_data = f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.weather_data_clean\"\n",
    "target_table_name_weather_units = f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.weather_data_units\"\n",
    "target_table_name_weather_forecast_data = f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.weather_forecast_clean\"\n",
    "\n",
    "raw_energy_df = spark.table(source_table_name_energy)\n",
    "raw_weather_history_df = spark.table(source_table_name_weather_history)\n",
    "raw_weather_forecast_df = spark.table(source_table_name_weather_forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3ee18a5-9aeb-4ef6-ab27-6420afd05c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Energy data clean up\n",
    "From our exploration we had the following statistics:\n",
    "\n",
    "- Total count before processing   =  2,154,498,882\n",
    "- Number of missing datapoints is =  10,979,368 (0.51%)\n",
    "\n",
    "after the processing below we arrive at = 2,142,619,142"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "934c58ee-c5d9-4514-b304-105e3fb5ae42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def round_to_nearest_half_hour(ts_col):\n",
    "    return F.from_unixtime((F.unix_timestamp(ts_col) / 1800).cast(\"integer\") * 1800)\n",
    "\n",
    "clean_meter_df = (\n",
    "    raw_energy_df\n",
    "    .filter(\"total_consumption_active_import <= 200000 AND total_consumption_active_import >= 0\")\n",
    "    .dropna(how='any', subset=['aggregated_device_count_active', 'total_consumption_active_import'])\n",
    "    .withColumn(\n",
    "        \"data_collection_log_timestamp\",\n",
    "        round_to_nearest_half_hour(\"data_collection_log_timestamp\").cast('timestamp'),\n",
    "    )\n",
    "    .groupby(\"lv_feeder_unique_id\", \"data_collection_log_timestamp\")\n",
    "    .agg(\n",
    "      F.mean(\"total_consumption_active_import\").alias(\"total_consumption_active_import\"),\n",
    "      F.first('aggregated_device_count_active').alias('aggregated_device_count_active'),\n",
    "      F.first('geometry').alias('geometry'),\n",
    "      F.first('secondary_substation_unique_id').alias('secondary_substation_unique_id'),\n",
    "      F.first('dataset_id').alias('dataset_id'),\n",
    "      F.first('dno_alias').alias('dno_alias')\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4874804-b701-4cc9-8800-c7c2b2da772b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Â Weather data shaping\n",
    "\n",
    "Data comes in with the cardinality:\n",
    "1 row per variable reading per time period for a set of coordinates.\n",
    "\n",
    "We need to pivot this into:\n",
    "1 row per time period, coordinate combination with each variable value in a column.\n",
    "\n",
    "We're also going to extract the variable units from the metadata to store as reference incase we need them. Unfortunately, the map keys are a little complex. They're all of the form `variable_name#field_name`. Luckily we can still just use getItem(), or `[]`, we just need to think about dynamically creating the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c0756f3-0a99-439e-bbc2-7610975b4151",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "historic_weather_pivoted_df = (\n",
    "    raw_weather_history_df.groupBy(\"time\", \"x\", \"y\")\n",
    "    .pivot(\"variable\", [\"t2m\", \"u10\", \"v10\", \"ssrd\", \"strd\"])\n",
    "    .agg(F.first(\"m\").alias(\"m\"))\n",
    ")\n",
    "\n",
    "weather_units_df = raw_weather_history_df.withColumn(\n",
    "    \"variable_units\",\n",
    "    # Get the value from the map for the key  '<variable>#units'\n",
    "    F.col(\"metadata\").getItem(F.concat(F.col(\"variable\"), F.lit(\"#units\")))\n",
    ").groupby('variable').agg(F.first('variable_units').alias('units'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ac0eeed-d67b-43be-9bb1-80cd421b3dec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_weather_pivoted_df = (\n",
    "    raw_weather_forecast_df.groupBy(\"valid_time\", \"x\", \"y\")\n",
    "    .pivot(\"variable\", [\"t2m\", \"u10\", \"v10\", \"ssrd\", \"strd\"])\n",
    "    .agg(F.first(\"m\").alias(\"m\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53f3d4c3-52a6-4060-ab05-f0fd4f4bbc12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Writing the data out\n",
    "\n",
    "Write the data out to storage. If you don't want to write this data again, you can comment out this cell entirely and include it in the next notebook, 03_feature_engineering, with a `%run` command. You will need to comment out the cell titled 'Load data and configs'. It _should_ work out of the box, but double check the variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d9fa503-0088-4045-9f4b-7752deb5796a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tables_to_process = {\n",
    "    target_table_name_energy_data: clean_meter_df,\n",
    "    target_table_name_weather_history_data: historic_weather_pivoted_df,\n",
    "    target_table_name_weather_units: weather_units_df,\n",
    "    target_table_name_weather_forecast_data: forecast_weather_pivoted_df,\n",
    "}\n",
    "  \n",
    "for tgt, df_to_process in tables_to_process.items():\n",
    "    if spark.catalog.tableExists(tgt) and (not CONFIG.overwrite_data):\n",
    "        print(f\"Skipping table {tgt} as it already exists and overwrite is not set\")\n",
    "        continue\n",
    "    df_to_process.write.saveAsTable(tgt, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57323b04-6ce9-4498-b728-0eb44ad17a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 695471856120657,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2,
    "widgetLayout": []
   },
   "notebookName": "02_data_prep_for_feature_engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
