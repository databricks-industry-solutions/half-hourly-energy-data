{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45027246-ce88-45ce-94cb-7fdd9159c0ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# 01 - Acquiring historical weather data\n",
    "In this notebook, we will obtain historical weather data for the period of interest (Jan 2024 until now) produced by the [European Centre for Medium-Range Weather Forecasts (ECMWF)](https://www.ecmwf.int/) through their [Climate Data Store (CDS) service](https://cds.climate.copernicus.eu/).\n",
    "\n",
    "This data is freely available but subject to registration and limits on the size of each request for data submitted to the service. \n",
    "\n",
    "Register for an account at the [ECMWF site](https://accounts.ecmwf.int/auth/realms/ecmwf/login-actions/registration) and log into the CDS site and check under \"Your Profile\" to generate an API token. Copy your token and store it as a [Databricks Secret](https://docs.databricks.com/aws/en/security/secrets/example-secret-workflow). Update the `secret_scope` and `secret_key` variables in the workflow configuration to reflect (see notebook `00_project_setup`).\n",
    "\n",
    "We're looking at the middle row of this diagram:\n",
    "\n",
    "<img src=\"../docs/imgs/energy-sa-ingest-flow.png\" width=\"800\">\n",
    "\n",
    "#### What does this notebook do? / What will you learn?\n",
    "\n",
    "Part 1:  Obtain data from the CDS API\n",
    "- Break down our time horizon and desired parameters into a number of smaller queries to avoid over-loading the API.\n",
    "- Use requests to submit the calls and retrieve the data.\n",
    "- Store the data in volumes.\n",
    "- Defining a custom spark data source to handle new formats\n",
    "\n",
    "Part 2:\n",
    "- Learning about the netCDF data type\n",
    "- Defining a custom spark data source\n",
    "- Loading our data from a volume\n",
    "\n",
    "Part 3:\n",
    "- Save the results\n",
    "\n",
    "#### Data licensing\n",
    "We are obliged to state that the ECMWF has not in any way endorsed this demo and remind you to fully read and comply with to their [terms of use](https://apps.ecmwf.int/datasets/licences/general/) when accessing the data.\n",
    "\n",
    "Generated using Copernicus Climate Change Service information 2025.\n",
    "\n",
    "ECMWF Open Data is © 2025 European Centre for Medium-Range Weather Forecasts (ECMWF).\n",
    "\n",
    "This data is published under a Creative Commons Attribution 4.0 International (CC BY 4.0). https://creativecommons.org/licenses/by/4.0/\n",
    "\n",
    "ECMWF does not accept any liability whatsoever for any error or omission in the data, their availability, or for any loss or damage arising from their use.\n",
    "\n",
    "The material presented here has not been modified from its original form.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c5e1aca-edfd-4f9f-94de-102792318251",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./includes/common_functions_and_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95da7ac7-1c42-44cb-9c24-ee6e6c0717df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import cdsapi\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54ce7156-ee58-4c38-b377-2cb5ad70fa4e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "General notebook setup"
    }
   },
   "outputs": [],
   "source": [
    "target_table_name = f\"{CONFIG.target_catalog}.{CONFIG.target_schema}.weather_data_raw\"\n",
    "target_volume_path = f\"/Volumes/{CONFIG.target_catalog}/{CONFIG.target_schema}/unstructured_data\"\n",
    "download_root = os.path.join(target_volume_path, \"weather\")\n",
    "\n",
    "if spark.catalog.tableExists(target_table_name) and (not CONFIG.overwrite_data):\n",
    "  dbutils.notebook.exit('Target table exists and config is not set to overwrite, skipping notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae58bae3-a1ed-4413-9c22-634ffacba314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Accessing the Climate Data Store\n",
    "\n",
    "CDS comes with its own Python API (`CDSAPI`) for coordinating requests and downloads and we'll use it here to orchestrate the data acquisition process.\n",
    "\n",
    "We'll use this bounding box to constrain our CDS query (rounded to the most generous 0.25°) and limit the volume of data we need to handle.\n",
    "\n",
    "> United Kingdom bounding box:\n",
    "> + North: 61.25°\n",
    "> + East: 1.75°\n",
    "> + South: 49.75°\n",
    "> + West: -8.25°\n",
    "\n",
    "<img src=\"../docs/imgs/ukbbox.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42a64783-ae72-4999-a4a2-7492838e6884",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query_bounding_box=[61.25, -8.25, 49.75, 1.75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c815a49-7f93-4af0-9d41-b2521e3bed40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We need to instantiate the client with our token, then construct a series of requests in order to obtain the data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70082989-ee19-407f-8b10-752d5e5787cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "c = cdsapi.Client(\n",
    "    url=\"https://cds.climate.copernicus.eu/api\",\n",
    "    key=dbutils.secrets.get(CONFIG.secret_scope, CONFIG.secret_key),\n",
    ")\n",
    "\n",
    "dataset = \"reanalysis-era5-single-levels\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "664e8762-045a-4e6c-9dff-a3a9f91546ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We'll split our time horizon into three parts to allow us to successfully download the entire dataset without hitting any request limits.\n",
    "\n",
    "We'll also consider \"instantaneous\" and \"accumulative\" weather variables separately [(see these helpful definitions to understand the different between these)](https://confluence.ecmwf.int/pages/viewpage.action?pageId=85402030#heading-Instantaneousaccumulatedmeanrateandminmaxparameters).\n",
    "\n",
    "The variables we're interested are described in the table below:\n",
    "\n",
    "| Variable | I/A | Name | Units | Definition |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 2t   | I | 2m_temperature | K | [2 metre temperature](https://apps.ecmwf.int/codes/grib/param-db/167) |\n",
    "| 10u  | I | 10m_u_component_of_wind | m s-1 | [10 metre U wind component](https://apps.ecmwf.int/codes/grib/param-db/165) |\n",
    "| 10v  | I | 10m_v_component_of_wind | m s-1 | [10 metre V wind component](https://apps.ecmwf.int/codes/grib/param-db/166) |\n",
    "| ssrd | A | surface_solar_radiation_downwards | J m**-2 | [Surface solar radiation downwards](https://apps.ecmwf.int/codes/grib/param-db/169) |\n",
    "| strd | A | surface_thermal_radiation_downwards | J m**-2 | [Surface thermal radiation downwards](https://apps.ecmwf.int/codes/grib/param-db/175) |\n",
    "\n",
    "We use `itertools.product(intervals, variable_sets)` to get all permutations of download links that we need, like this:\n",
    "\n",
    "<img src=\"../docs/imgs/itertools_product.png\" width=\"1440\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f22c2207-5972-4d24-acbc-524d1cd4dcf2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parameterise our API calls"
    }
   },
   "outputs": [],
   "source": [
    "months = [f\"{m:02}\" for m in range(1, 13)]\n",
    "\n",
    "intervals = {\n",
    "  \"2024H1\": {\"year\": [\"2024\"], \"month\": months[:6]},\n",
    "  \"2024H2\": {\"year\": [\"2024\"], \"month\": months[6:]},\n",
    "  \"2025H1\": {\"year\": [\"2025\"], \"month\": months[:6]},\n",
    "  }\n",
    "variable_sets = {\n",
    "  \"instantaneous\": {\"variable\": [\"2t\", \"10u\", \"10v\"]},\n",
    "  \"accumulative\": {\"variable\": [\"ssrd\", \"strd\",]},\n",
    "}\n",
    "\n",
    "downloads = list(itertools.product(intervals.items(), variable_sets.items()))\n",
    "\n",
    "# # View the structure of our download request parameters here.\n",
    "# for i, v in downloads:\n",
    "#   print(i[0])\n",
    "#   print(f'intervals = {i}')\n",
    "#   print(f'variables = {v}\\n')\n",
    "\n",
    "# Define a base dictionary of common parameters we will use as a template\n",
    "request_base = {\n",
    "  \"product_type\": [\"reanalysis\"],\n",
    "  \"day\": [f\"{d:02}\" for d in range(1, 32)],\n",
    "  \"time\": [f\"{h:02}:00\" for h in range(0, 24)],\n",
    "  \"data_format\": \"netcdf\",\n",
    "  \"download_format\": \"unarchived\",\n",
    "  \"area\": query_bounding_box\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b511f8eb-30d9-4be7-a281-519970df8b23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's go ahead and execute those downloads now. The resulting data files will be stored in a UC Volume.\n",
    "\n",
    "We could attempt to parallelise the request / download process (using e.g. multithreading) but, to prevent high volumes of requests breaking the CDS platform, our requests will be queued and executed sequentially anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b881c4fb-2a83-4e9f-a527-efc0e0743eec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Make API calls sequentially"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(download_root)\n",
    "\n",
    "for interval, parameter_set in downloads:\n",
    "  # Unpack our tuples\n",
    "  interval_name, interval_values = interval \n",
    "  variable_name, variable_values = parameter_set\n",
    "  target_path = f\"{download_root}/{interval_name}-{variable_name}.nc\"\n",
    "  # If we arn't set to overwrite data and the file already exists, skip\n",
    "  if not CONFIG.overwrite_data and any(dbutils.fs.ls(target_path)):\n",
    "    print(f\"File: {target_path} exists, skipping...\")\n",
    "    continue\n",
    "  request = deepcopy(request_base)\n",
    "  request.update(interval_values)\n",
    "  request.update(variable_values)\n",
    "  print(f'Requesting data for {interval_name} and {variable_name}')\n",
    "  c.retrieve(dataset, request, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7b0214c-e289-48ac-9331-a6acdfea1ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Loading data from files downloaded from CDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f37ee0d8-5536-4cee-b470-cba53dee8168",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### NetCDF, a brief introduction\n",
    "Now we have data downloaded and stored away safely in a Volume. However, we still have an obstacle to using this data directly in that it is stored in an unusual format.\n",
    "\n",
    "Rather that publishing giant CSV files full of weather data coordinates, timestamps and values, meteorological organisations like ECMWF tend to publish their data in specialist scientific data formats such as [NetCDF](https://www.unidata.ucar.edu/software/netcdf/) or [GRIB](https://confluence.ecmwf.int/display/CKB/What+are+GRIB+files+and+how+can+I+read+them).\n",
    "\n",
    "Both are designed to efficiently store multidimensional array data, e.g. 'grids' of values, or collections of these grids. By convention, these grids are usually two-dimensional (but not always!) with the dimensions representing spatial dimensions such as longitude and latitude at regular intervals. In our case these intervals are 0.25 of a degree in both dimensions.\n",
    "\n",
    "In the case of NetCDF, the format we are working with here, the files are also inherently hierarchical:\n",
    "\n",
    "```\n",
    ".\n",
    "├── subdataset 1: 2 metre temperature\n",
    "├── subdataset 2: 'U' component windspeed\n",
    "│   ├── band 1: 2024-01-01T00:00:00.000Z\n",
    "│   ├── band 2: 2024-01-01T01:00:00.000Z\n",
    "│   ├── band_...\n",
    "│   └── band N: 2025-03-30T23:00:00.000Z\n",
    "├── subdataset ...\n",
    "├── subdataset N\n",
    "```\n",
    "\n",
    "+ Data for each weather variable is stored in a 'subdataset', a logically separated partition of the data within the file.\n",
    "+ Data for each forecast timestep within a subdataset is contained within a 'band'. Sequential bands represent the state of the variable on a regular spatial grid for sequential time steps.\n",
    "\n",
    "The files are therefore often very large and, in an ideal world, we would just use Spark to read the values from the inside of these files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4063b3f-6bef-40dd-8217-d05dfb2e4b9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Defining a custom Spark DataSource\n",
    "\n",
    "To save space in this notebook, we packaged this up into the includes folder. If you're interested in the source code, follow the link from the `%run` cell below.\n",
    "\n",
    "If you just want to see it in action, keep reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85c7adb6-e343-4e31-be10-94ada83b015b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./includes/custom_spark_data_source_era5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12457d4f-17f9-45f9-928e-120762392938",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Register our custom source with Spark"
    }
   },
   "outputs": [],
   "source": [
    "# Once we have a customer reader correctly set up, we need to register this reader with Spark.\n",
    "spark.dataSource.register(ERA5DataSource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2ad747f-a251-4afc-be82-da1c4760cc7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can now read this data in with Spark just as though it _is_ a big ol' CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65f68367-1dd6-4f84-9b37-d67b1d245ee9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read the data!"
    }
   },
   "outputs": [],
   "source": [
    "raw_df = (\n",
    "  spark.read\n",
    "  .format(\"era5\")\n",
    "  .load(download_root)\n",
    "  )\n",
    "raw_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7e37538-57bf-4108-bdb4-8742b35839e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: Saving the results\n",
    "\n",
    "Now we have our dataframe, we're going to grab the variable indicator from the subdataset name (Final string value following the `:` after filename.nc) and save our table to UC.\n",
    "\n",
    "**Note** Depending on your cluster size, this can take some time. We need to process 87M rows of data with a high-degree of partitioning. You will end up with around 54887 tasks to run.\n",
    "\n",
    "The typical task time is ~1.5-2 seconds,  which with a 32 core cluster will equate to somewhere between 43 and 57 minutes to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289503e0-de96-437e-b040-1b22e218f031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f'Saving era5 data to: {target_table_name}')\n",
    "(\n",
    "  raw_df.withColumn(\n",
    "      \"variable\", F.reverse(F.split(\"subdataset\", \":\")).getItem(0)\n",
    "  ).write.saveAsTable(target_table_name, mode=\"overwrite\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96c2e2d6-7c6e-4acf-b7b4-fcc31c5a2092",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Onto data pre-processing and analysis!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3394427869891594,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "01b_ingest_historic_weather_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
