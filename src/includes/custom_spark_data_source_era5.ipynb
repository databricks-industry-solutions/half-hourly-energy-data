{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68dd5acc-1c03-4c5e-bb99-1c29d6fa1de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from pyspark.sql.datasource import DataSource, DataSourceReader, InputPartition\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, MapType, LongType, TimestampType\n",
    "from datetime import datetime, UTC\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c98a357-ee39-4935-8fa7-0ee738dc4049",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Design decision: When to split files?\n",
    "\n",
    "Step one is to think about how we might want to parallise / partition the read process. By default, Spark will create a task per input file it reads, but we have a relatively small number of files to work with here and reading any one of these into memory in its entirety might result in an out-of-memory situation.\n",
    "\n",
    "The good news is: we can take advantage of the hierarchical nature of these files to parallelise the read process and scale it out across a large cluster. All we need to do is extend the `pyspark.sql.datasource.InputPartition` class and implement a method called `partitions` in our `DataSourceReader` that exposes the full set of partitions to be read.\n",
    "\n",
    "We'll partition across both **subdataset** and **band**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15899d2c-187b-460b-960c-18427c947adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class SubdatasetBandPartition(InputPartition):\n",
    "  def __init__(self, subdataset: str, band: int):\n",
    "    self.subdataset = subdataset\n",
    "    self.band = band"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c038470f-754c-423f-ab70-39e58d4f36dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here's our `ERA5DataSourceReader` class. It uses the `rasterio` library to read the metadata and contents of each file and yield a 7-tuple per data point inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a7e1d2c-744f-4b51-82ec-66ae5c65e0dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ERA5DataSourceReader(DataSourceReader):\n",
    "  def __init__(self, schema, options):\n",
    "    self.schema: StructType = schema\n",
    "    self.options = options\n",
    "\n",
    "  @staticmethod\n",
    "  def unix_to_datetime(unix_timestamp: int) -> datetime:\n",
    "    return datetime.fromtimestamp(unix_timestamp, UTC)\n",
    "  \n",
    "  @staticmethod\n",
    "  def convert_longitude_0_360_to_180(lon_array):\n",
    "    \"\"\"\n",
    "    Convert longitude from 0-360째 range to -180 to +180째 range\n",
    "    \n",
    "    Parameters:\n",
    "    lon_array (numpy.ndarray): Array of longitudes in 0-360째 range\n",
    "    \n",
    "    Returns:\n",
    "    numpy.ndarray: Array of longitudes in -180 to +180째 range\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original array\n",
    "    converted = np.copy(lon_array)\n",
    "    \n",
    "    # Find values > 180 and subtract 360 from them\n",
    "    converted = np.where(converted > 180, converted - 360, converted)\n",
    "    \n",
    "    return converted\n",
    "  \n",
    "  @property\n",
    "  def input_path(self):\n",
    "    path: str = self.options.get(\"path\")\n",
    "    if not path:\n",
    "      raise ValueError(\"The 'path' option is required.\")\n",
    "    return path\n",
    "  \n",
    "  @property\n",
    "  def input_files(self):\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(self.input_path):\n",
    "      for filename in filenames:\n",
    "        if filename.endswith(\".nc\"):\n",
    "          files.append(os.path.join(root, filename))\n",
    "    return files\n",
    "  \n",
    "  def partitions(self):\n",
    "    \"\"\"For each file, use rasterio to open and grab the relevant hierarchy of data and create a list of partitions using our custom partitioning class: SubdatasetBandPartition(subdataset, band)\"\"\"\n",
    "    parts: list[SubdatasetBandPartition] = []\n",
    "    for path in self.input_files:\n",
    "      with rasterio.open(path) as src:\n",
    "        for sd in src.subdatasets:\n",
    "          with rasterio.open(sd) as r:\n",
    "            parts += [SubdatasetBandPartition(sd, b + 1) for b in range(r.count)]\n",
    "    return parts\n",
    "\n",
    "  def read(self, partition):\n",
    "    \"\"\"For each partition, use rasterio to open and grab the relevant hierarchy of data and return a generator of tuples containing the subdataset, metadata, band index, coordinates, and values\"\"\"\n",
    "    # In case of an empty directory or no files found\n",
    "    if not getattr(partition, 'subdataset', None):\n",
    "      return\n",
    "    with rasterio.open(partition.subdataset) as r:\n",
    "      metadata_tags = r.tags()\n",
    "      del metadata_tags[\"NETCDF_DIM_valid_time_VALUES\"]\n",
    "      image_height, image_width = r.shape\n",
    "      cols, rows = np.meshgrid(np.arange(image_width), np.arange(image_height))\n",
    "      xs, ys = rasterio.transform.xy(r.transform, rows, cols) # , offset=\"ul\"\n",
    "      timesteps = r.tags()[\"NETCDF_DIM_valid_time_VALUES\"][1:-1].split(\",\")\n",
    "      valid_time = self.unix_to_datetime(int(timesteps[partition.band - 1]))\n",
    "      values = r.read(partition.band)\n",
    "      value_count = len(values)\n",
    "      rows = zip(\n",
    "        self.convert_longitude_0_360_to_180(xs),\n",
    "        ys,\n",
    "        values.astype(np.float32).flatten()\n",
    "        )\n",
    "      for rw in rows:\n",
    "          yield (partition.subdataset, metadata_tags, partition.band, valid_time, *rw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44dd5617-8616-4818-bccf-9e7b7dd18013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Create our ERA5DataSource**\n",
    "\n",
    "we need to combine our reader, which controls distrubition and reading logic, with a Spark `DataSource` abstraction which tells Spark the schema of the data expected to be returned by the reader, and gives it a freindly name to be used in the call to SparkSession.read\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bce98c4-e029-4fb1-885d-4b25779918ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class ERA5DataSource(DataSource):\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls) -> str:\n",
    "        \"\"\"\n",
    "        Get the name of the data source.\n",
    "\n",
    "        Returns:\n",
    "            str: The name of the data source.\n",
    "        \"\"\"\n",
    "        return \"era5\"\n",
    "    \n",
    "    def schema(self) -> StructType:\n",
    "        \"\"\"\n",
    "        Define the schema for the output data.\n",
    "\n",
    "        Returns:\n",
    "            StructType: The schema including fields for the variable identifier, band index,\n",
    "            metadata, coordinates, and values.\n",
    "        \"\"\"\n",
    "        return StructType([\n",
    "            StructField(\"subdataset\", StringType(), True),\n",
    "            StructField(\"metadata\", MapType(StringType(), StringType()), True),\n",
    "            StructField(\"band\", LongType(), True),\n",
    "            StructField(\"time\", TimestampType(), True),\n",
    "            StructField(\"x\", DoubleType(), True),\n",
    "            StructField(\"y\", DoubleType(), True),\n",
    "            StructField(\"m\", DoubleType(), True),\n",
    "        ])\n",
    "\n",
    "    def reader(self, schema: StructType):\n",
    "        return ERA5DataSourceReader(schema, self.options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4aec7be-92af-43a8-98f1-05e7ff654c45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Register and use this datasource with:\n",
    "# spark.dataSource.register(ERA5DataSource)\n",
    "#spark.read.format(\"era5\").load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6612cba5-90f7-474c-8ee3-f3a8324262d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "custom_spark_data_source_era5",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
